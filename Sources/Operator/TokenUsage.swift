import LLM

/// Normalized token usage across LLM providers.
///
/// Different providers report usage with different field names.
/// `TokenUsage` normalizes them into a single shape.
public struct TokenUsage: Friendly {
    /// Tokens consumed by the prompt (input).
    public var promptTokens: Int

    /// Tokens generated by the model (output).
    public var completionTokens: Int

    /// Total tokens (prompt + completion).
    public var totalTokens: Int

    /// Creates a token usage record with the given counts.
    public init(promptTokens: Int, completionTokens: Int, totalTokens: Int) {
        self.promptTokens = promptTokens
        self.completionTokens = completionTokens
        self.totalTokens = totalTokens
    }

    /// A usage value of zero across all fields.
    public static let zero = TokenUsage(promptTokens: 0, completionTokens: 0, totalTokens: 0)

    /// Adds two usage records together, summing each field.
    public static func + (lhs: TokenUsage, rhs: TokenUsage) -> TokenUsage {
        TokenUsage(
            promptTokens: lhs.promptTokens + rhs.promptTokens,
            completionTokens: lhs.completionTokens + rhs.completionTokens,
            totalTokens: lhs.totalTokens + rhs.totalTokens
        )
    }

    /// Creates a `TokenUsage` from an LLM provider's raw usage response.
    ///
    /// Normalizes OpenAI (`prompt_tokens`/`completion_tokens`) and
    /// Anthropic (`input_tokens`/`output_tokens`) into a single shape.
    public static func from(
        _ usage: LLM.OpenAICompatibleAPI.ChatCompletionResponse.Usage
    ) -> TokenUsage {
        let prompt = usage.prompt_tokens ?? usage.input_tokens ?? 0
        let completion = usage.completion_tokens ?? usage.output_tokens ?? 0
        let total = usage.total_tokens ?? (prompt + completion)
        return TokenUsage(promptTokens: prompt, completionTokens: completion, totalTokens: total)
    }
}
